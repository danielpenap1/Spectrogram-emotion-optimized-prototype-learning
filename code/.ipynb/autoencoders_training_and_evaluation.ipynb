{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8e3efc14",
      "metadata": {
        "id": "8e3efc14",
        "tags": []
      },
      "source": [
        "# Spectrogram Emotion Prototype Learning\n",
        "\n",
        "*Code fully written by Daniel Peña Porras*.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*    #### Training and evaluation of **CONVOLUTIONAL AUTOENCODER** with traditional and gradient-based loss functions to ensure preservation of highly detailed parts of the spectrograms.\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73a797d6",
      "metadata": {
        "id": "73a797d6",
        "outputId": "73d6ebef-f4f3-41c3-ab0c-6bf5480bdd5a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n@author: Daniel Peña Porras.\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "@author: Daniel Peña Porras.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62787479",
      "metadata": {
        "id": "62787479",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "##### Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "845b3aac",
      "metadata": {
        "id": "845b3aac",
        "outputId": "87754015-c9c5-460a-ac2b-bc6354b803aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting librosa\n",
            "  Downloading librosa-0.11.0-py3-none-any.whl (260 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.7/260.7 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting soxr>=0.3.2\n",
            "  Downloading soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.8/252.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.9.1)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.22.4)\n",
            "Collecting audioread>=2.1.9\n",
            "  Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.0.4)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.3.0)\n",
            "Collecting soundfile>=0.12.1\n",
            "  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pooch>=1.1\n",
            "  Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m233.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.1.0)\n",
            "Collecting lazy_loader>=0.1\n",
            "  Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.55.2)\n",
            "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from lazy_loader>=0.1->librosa) (23.2)\n",
            "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (65.3.0)\n",
            "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.38.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (4.1.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.28.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.1.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2022.6.15.1)\n",
            "Installing collected packages: soxr, lazy_loader, audioread, soundfile, pooch, librosa\n",
            "Successfully installed audioread-3.0.1 lazy_loader-0.4 librosa-0.11.0 pooch-1.8.2 soundfile-0.13.1 soxr-0.5.0.post1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting mir_eval\n",
            "  Downloading mir_eval-0.8.2-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.8/102.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.10/site-packages (from mir_eval) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from mir_eval) (1.9.1)\n",
            "Requirement already satisfied: decorator in /opt/conda/lib/python3.10/site-packages (from mir_eval) (5.1.1)\n",
            "Installing collected packages: mir_eval\n",
            "Successfully installed mir_eval-0.8.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install librosa\n",
        "! pip install mir_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b0e7a59",
      "metadata": {
        "id": "1b0e7a59",
        "outputId": "f6c8e98e-dcf1-4e83-8ffe-4ed07bd0dc4f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-19 07:48:29.871247: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.compat.v1.enable_eager_execution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c8d6b06",
      "metadata": {
        "id": "2c8d6b06"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import librosa\n",
        "import librosa.display as dsp\n",
        "\n",
        "import mir_eval\n",
        "\n",
        "from IPython.display import Audio\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from skimage.transform import resize\n",
        "from scipy.ndimage import gaussian_laplace\n",
        "\n",
        "from keras.callbacks import TensorBoard\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.layers import LeakyReLU, BatchNormalization, ActivityRegularization#, Sigmoid\n",
        "import os\n",
        "from keras import backend as K\n",
        "from tensorflow.keras.utils import plot_model, model_to_dot\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, Callback\n",
        "\n",
        "from skimage.transform import resize\n",
        "from keras.callbacks import Callback\n",
        "from keras.applications import VGG19, EfficientNetB0, ResNet50V2, DenseNet121\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Lambda, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import MeanSquaredError\n",
        "\n",
        "from scipy.ndimage import gaussian_laplace\n",
        "\n",
        "from scipy.io import wavfile\n",
        "\n",
        "from keras.constraints import NonNeg,UnitNorm\n",
        "from keras.initializers import Constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab70f5e0",
      "metadata": {
        "id": "ab70f5e0"
      },
      "outputs": [],
      "source": [
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ee0eaa",
      "metadata": {
        "id": "e7ee0eaa",
        "tags": []
      },
      "source": [
        "##### Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "298b1a7c",
      "metadata": {
        "id": "298b1a7c"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "DATA PARAMETERS\n",
        "\"\"\"\n",
        "#prepocess_window_size_ms = 100\n",
        "#preprocess_window_overlapping_ms = 20\n",
        "test_prctg = 0.3\n",
        "\n",
        "dataset = 'EmoMatch' # or 'EmoDB' 'RAVDESS'\n",
        "\n",
        "dtype = 'float32'\n",
        "n_frames_cnn = None #128 established after data loading\n",
        "n_freq_cnn = None #20 established after data loading\n",
        "n_color_channels = 1\n",
        "\n",
        "#same parameters as in preprocessing\n",
        "n_fft = 1024            # length of the FFT window\n",
        "window = 'hamming'      # window function\n",
        "win_length = 512        # window length\n",
        "hop_length = 256        # number of samples between successive frames\n",
        "n_mels = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f658505a",
      "metadata": {
        "id": "f658505a"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "ENCODER HYPERPARAMETERS\n",
        "\"\"\"\n",
        "n_filters_enc = [64, 32, 16, 8, 4]\n",
        "filters_size_cnn_enc = [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3)]\n",
        "strides_enc = [2, 1, 1, 1, 1]\n",
        "paddings_enc = ['same', 'same', 'same', 'same', 'same']\n",
        "kernel_initializer = 'he_uniform'\n",
        "use_batch_norm = False\n",
        "use_l1_reg = False\n",
        "conv_activation = 'relu'\n",
        "encoded_activation = 'relu'\n",
        "encoder_name = 'encoder'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bde8ade",
      "metadata": {
        "id": "4bde8ade"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "DECODER HYPERPARAMETERS\n",
        "  kernel_initializer, use_batch_norm and use_l1_reg are the same as in the encoder\n",
        "\"\"\"\n",
        "n_filters_dec = [4, 8, 16, 32, 64]\n",
        "filters_size_cnn_dec = [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)]\n",
        "strides_dec = [1, 1, 1, 1, 2]\n",
        "paddings_dec = ['same', 'same', 'same', 'same', 'same', 'same']\n",
        "deconv_activation = 'relu'\n",
        "decoded_activation = 'sigmoid'\n",
        "decoder_name = 'decoder'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64cb316f",
      "metadata": {
        "id": "64cb316f"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "TRAINING HYPERPARAMETERS\n",
        "\"\"\"\n",
        "\n",
        "autoencoder_epochs = 100\n",
        "autoencoder_batch_size = 128\n",
        "autoencoder_learning_rate = 0.001\n",
        "\n",
        "\n",
        "lambdaR = 1\n",
        "reconstruction_loss = keras.losses.MeanSquaredError()\n",
        "recons_loss_name = 'keras.losses.MeanSquaredError()'\n",
        "\n",
        "reconstruct_with_HFENN = True\n",
        "if reconstruct_with_HFENN:\n",
        "    lambdaHFENN = 0.0001\n",
        "    lambdaR = 1\n",
        "\n",
        "reconstruction_patience_epochs = 50\n",
        "reconstruction_convergence = 0\n",
        "\n",
        "ratioReconstructionDisplay = 10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if dataset == 'RAVDESS':\n",
        "    sr = 48000\n",
        "    n_speakers = 24\n",
        "    complete_speakers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
        "                         14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
        "    n_phonemes = 44\n",
        "    n_classes = 8\n",
        "    class_names = [\n",
        "        'neutral',\n",
        "        'calm',\n",
        "        'happy',\n",
        "        'sad',\n",
        "        'angry',\n",
        "        'fearful',\n",
        "        'disgust',\n",
        "        'surprised'\n",
        "    ]\n",
        "    get_prototype_emotion = {\n",
        "        0: 'neutral',\n",
        "        1: 'calm',\n",
        "        2: 'joy',\n",
        "        3: 'sadness',\n",
        "        4: 'anger',\n",
        "        5: 'fear',\n",
        "        6: 'disgust',\n",
        "        7: 'surprise'\n",
        "    }\n",
        "    male_speakers = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23]\n",
        "    female_speakers = [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24]\n",
        "\n",
        "elif dataset == 'EmoMatch':\n",
        "    sr = 48000\n",
        "    # n_speakers = 50\n",
        "    speakers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 20, 22, 23,\n",
        "                25, 29, 31, 33, 35, 36, 37, 40, 41, 43, 45, 46, 49]\n",
        "    n_speakers = len(n_speakers)\n",
        "    n_phonemes = 24\n",
        "    n_classes = 7\n",
        "    class_names = [\n",
        "        'angry',\n",
        "        'disgust',\n",
        "        'fearful',\n",
        "        'happy',\n",
        "        'neutral',\n",
        "        'sad',\n",
        "        'surprised'\n",
        "    ]\n",
        "    get_prototype_emotion = {\n",
        "        0: 'anger',\n",
        "        1: 'disgust',\n",
        "        2: 'fear',\n",
        "        3: 'joy',\n",
        "        4: 'neutral',\n",
        "        5: 'sadness',\n",
        "        6: 'surprise'\n",
        "    }\n",
        "    male_speakers = [1, 2, 5, 9, 12, 13, 22, 23, 31, 35, 36, 37, 43]\n",
        "    female_speakers = [3, 4, 6, 7, 8, 10, 11, 20, 25, 29, 33, 40, 41, 45, 46, 49]\n",
        "\n",
        "elif dataset == 'EmoDB':\n",
        "    sr = 16000\n",
        "    n_speakers = 10\n",
        "    speakers = [3, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
        "    n_phonemes = 45\n",
        "    n_classes = 7\n",
        "    class_names = [\n",
        "        'angry',\n",
        "        'bored',\n",
        "        'disgust',\n",
        "        'fearful',\n",
        "        'happy',\n",
        "        'neutral',\n",
        "        'sad'\n",
        "    ]\n",
        "    get_prototype_emotion = {\n",
        "        0: 'angry',\n",
        "        1: 'bored',\n",
        "        2: 'disgust',\n",
        "        3: 'fearful',\n",
        "        4: 'happy',\n",
        "        5: 'neutral',\n",
        "        6: 'sad'\n",
        "    }\n",
        "    male_speakers = [3, 10, 11, 12, 15]\n",
        "    female_speakers = [8, 9, 13, 14, 16]\n",
        "\n",
        "n_prototypes = n_phonemes * n_classes\n",
        "\n",
        "fmax = sr/2\n",
        "fmin = 0  # default value if not specified"
      ],
      "metadata": {
        "id": "bp2WT7cTqbfs"
      },
      "id": "bp2WT7cTqbfs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "rtA7EQfYOt6-",
      "metadata": {
        "id": "rtA7EQfYOt6-",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "##### Models and losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hKJl9a1OOvs2",
      "metadata": {
        "id": "hKJl9a1OOvs2"
      },
      "outputs": [],
      "source": [
        "def create_encoder(input_img,\n",
        "                    n_filters_enc,\n",
        "                    filters_size_cnn_enc,\n",
        "                    strides_enc,\n",
        "                    paddings_enc,\n",
        "                    kernel_initializer,\n",
        "                    use_batch_norm,\n",
        "                    use_l1_reg,\n",
        "                    conv_activation,\n",
        "                    encoded_activation,\n",
        "                    encoder_name):\n",
        "\n",
        "  x = K.expand_dims(input_img,-1)\n",
        "  l1 = layers.Conv2D(n_filters_enc[0], filters_size_cnn_enc[0], padding=paddings_enc[0], strides=strides_enc[0], kernel_initializer=kernel_initializer, activation=conv_activation, name='conv1')(x)\n",
        "  if use_l1_reg:\n",
        "    l1 = ActivityRegularization(l1=10e-10)(l1)\n",
        "  if use_batch_norm:\n",
        "    l1 = BatchNormalization()(l1)\n",
        "  l2 = layers.Conv2D(n_filters_enc[1], filters_size_cnn_enc[1], padding=paddings_enc[1], strides=strides_enc[1], kernel_initializer=kernel_initializer, activation=conv_activation, name='conv2')(l1)\n",
        "  if use_l1_reg:\n",
        "    l2 = ActivityRegularization(l1=10e-10)(l2)\n",
        "  if use_batch_norm:\n",
        "    l2 = BatchNormalization()(l2)\n",
        "  l3 = layers.Conv2D(n_filters_enc[2], filters_size_cnn_enc[2], padding=paddings_enc[2], strides=strides_enc[2], kernel_initializer=kernel_initializer, activation=conv_activation, name='conv3')(l2)\n",
        "  if use_l1_reg:\n",
        "    l3 = ActivityRegularization(l1=10e-10)(l3)\n",
        "  if use_batch_norm:\n",
        "    l3 = BatchNormalization()(l3)\n",
        "  l4 = layers.Conv2D(n_filters_enc[3], filters_size_cnn_enc[3], padding=paddings_enc[3], strides=strides_enc[3], kernel_initializer=kernel_initializer, activation=conv_activation, name='conv4')(l3)\n",
        "  if use_l1_reg:\n",
        "    l4 = ActivityRegularization(l1=10e-10)(l4)\n",
        "  if use_batch_norm:\n",
        "    l4 = BatchNormalization()(l4)\n",
        "  encoded = layers.Conv2D(n_filters_enc[4], filters_size_cnn_enc[4], padding=paddings_enc[4], strides=strides_enc[4], kernel_initializer=kernel_initializer, activation=encoded_activation, name='encoded')(l4)\n",
        "  if use_l1_reg:\n",
        "    encoded = ActivityRegularization(l1=10e-10)(encoded)\n",
        "  if use_batch_norm:\n",
        "    encoded = BatchNormalization()(encoded)\n",
        "\n",
        "  return Model(input_img, encoded, name=encoder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Md-EkrRPPKDl",
      "metadata": {
        "id": "Md-EkrRPPKDl"
      },
      "outputs": [],
      "source": [
        "def create_decoder(decoder_input,\n",
        "                    n_color_channels,\n",
        "                    n_filters_dec,\n",
        "                    filters_size_cnn_dec,\n",
        "                    strides_dec,\n",
        "                    paddings_dec,\n",
        "                    kernel_initializer,\n",
        "                    use_batch_norm,\n",
        "                    use_l1_reg,\n",
        "                    deconv_activation,\n",
        "                    decoded_activation,\n",
        "                    decoder_name):\n",
        "\n",
        "  l1 = layers.Conv2DTranspose(n_filters_dec[0], filters_size_cnn_dec[0], padding=paddings_dec[0], strides=strides_dec[0], kernel_initializer=kernel_initializer, activation=deconv_activation, name='deconv1')(decoder_input)\n",
        "  if use_l1_reg:\n",
        "    l1 = ActivityRegularization(l1=10e-10)(l1)\n",
        "  if use_batch_norm:\n",
        "    l1 = BatchNormalization()(l1)\n",
        "  l2 = layers.Conv2DTranspose(n_filters_dec[1], filters_size_cnn_dec[1], padding=paddings_dec[1], strides=strides_dec[1], kernel_initializer=kernel_initializer, activation=deconv_activation, name='deconv2')(l1)\n",
        "  if use_l1_reg:\n",
        "    l2 = ActivityRegularization(l1=10e-10)(l2)\n",
        "  if use_batch_norm:\n",
        "    l2 = BatchNormalization()(l2)\n",
        "  l3 = layers.Conv2DTranspose(n_filters_dec[2], filters_size_cnn_dec[2], padding=paddings_dec[2], strides=strides_dec[2], kernel_initializer=kernel_initializer, activation=deconv_activation, name='deconv3')(l2)\n",
        "  if use_l1_reg:\n",
        "    l3 = ActivityRegularization(l1=10e-10)(l3)\n",
        "  if use_batch_norm:\n",
        "    l3 = BatchNormalization()(l3)\n",
        "  l4 = layers.Conv2DTranspose(n_filters_dec[3], filters_size_cnn_dec[3], padding=paddings_dec[3], strides=strides_dec[3], kernel_initializer=kernel_initializer, activation=deconv_activation, name='deconv4')(l3)\n",
        "  if use_l1_reg:\n",
        "    l4 = ActivityRegularization(l1=10e-10)(l4)\n",
        "  if use_batch_norm:\n",
        "    l4 = BatchNormalization()(l4)\n",
        "  l5 = layers.Conv2DTranspose(n_filters_dec[4], filters_size_cnn_dec[4], padding=paddings_dec[4], strides=strides_dec[4], kernel_initializer=kernel_initializer, activation=deconv_activation, name='deconv5')(l4)\n",
        "  if use_l1_reg:\n",
        "    l5 = ActivityRegularization(l1=10e-10)(l5)\n",
        "  if use_batch_norm:\n",
        "    l5 = BatchNormalization()(l5)\n",
        "\n",
        "  y = layers.Conv2D(n_color_channels, filters_size_cnn_dec[5], padding=paddings_dec[5], activation=decoded_activation)(l5)\n",
        "  decoded = K.squeeze(y, axis=-1)\n",
        "\n",
        "  return Model(decoder_input, decoded, name=decoder_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iXsLhFU-PYC2",
      "metadata": {
        "id": "iXsLhFU-PYC2"
      },
      "outputs": [],
      "source": [
        "def hfenn(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes the HFENN (High-Frequency Error Norm) loss between y_true and y_pred.\n",
        "\n",
        "    Arguments:\n",
        "    y_true -- Ground truth tensor (batch_size, height, width, channels)\n",
        "    y_pred -- Predicted tensor (batch_size, height, width, channels)\n",
        "\n",
        "    Returns:\n",
        "    hfenn_loss -- HFENN loss\n",
        "    \"\"\"\n",
        "\n",
        "    y_true = K.expand_dims(y_true,-1)\n",
        "    y_pred = K.expand_dims(y_pred,-1)\n",
        "\n",
        "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
        "\n",
        "    # Compute gradients of y_true and y_pred\n",
        "    grad_y_true = tf.image.sobel_edges(y_true)\n",
        "    grad_y_pred = tf.image.sobel_edges(y_pred)\n",
        "\n",
        "    # Compute HFENN loss\n",
        "    hfenn_loss = K.mean(K.abs(grad_y_true - grad_y_pred))\n",
        "\n",
        "    return hfenn_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40be7729",
      "metadata": {
        "id": "40be7729",
        "tags": []
      },
      "source": [
        "## **Training loop**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for speaker_id in range(1, n_speakers + 1):\n",
        "for speaker in speakers:\n",
        "    speaker = f\"{speaker_id:02d}\"  # format as '01', '02', ..., '50'\n",
        "    print(f\"\\n\\n🔁 Training Autoencoder for Speaker {speaker}...\")\n",
        "\n",
        "    # Define paths\n",
        "    data_path = f'/home/jovyan/Daniel/Exp_{dataset}/datasets/preprocessed/{dataset}100ms32LabeledSpe{speaker}.npz'\n",
        "    store_AE_path = f'/home/jovyan/Daniel/Exp_{dataset}/autoencoders_training/models/autoencoder_Speaker{speaker}.keras'\n",
        "    store_encoder_path = f'/home/jovyan/Daniel/Exp_{dataset}/autoencoders_training/models/encoder_Speaker{speaker}.keras'\n",
        "    store_decoder_path = f'/home/jovyan/Daniel/Exp_{dataset}/autoencoders_training/models/decoder_Speaker{speaker}.keras'\n",
        "\n",
        "    # Load data\n",
        "    data = np.load(data_path)\n",
        "    x_train_100ms = data['x_train']\n",
        "    y_train_100ms = data['y_emo']\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        x_train_100ms, y_train_100ms, test_size=test_prctg, random_state=42)\n",
        "\n",
        "    # Normalization\n",
        "    min_norm = np.min(x_train)\n",
        "    max_norm = np.max(x_train)\n",
        "\n",
        "    x_train_norm = (x_train - min_norm) / (max_norm - min_norm)\n",
        "    x_test_norm = (x_test - min_norm) / (max_norm - min_norm)\n",
        "\n",
        "    # Define model\n",
        "    input_img = Input(shape=(x_train.shape[1], x_train.shape[2]), dtype=x_train.dtype, name='input')\n",
        "    encoder = create_encoder(input_img, n_filters_enc, filters_size_cnn_enc, strides_enc, paddings_enc,\n",
        "                             kernel_initializer, use_batch_norm, use_l1_reg, conv_activation,\n",
        "                             encoded_activation, encoder_name)\n",
        "    encoded = encoder(input_img)\n",
        "\n",
        "    decoder_input = Input(shape=encoded.shape[1:], dtype=x_train.dtype, name='decoder_input')\n",
        "    decoder = create_decoder(decoder_input, n_color_channels, n_filters_dec, filters_size_cnn_dec,\n",
        "                             strides_dec, paddings_dec, kernel_initializer, use_batch_norm,\n",
        "                             use_l1_reg, deconv_activation, decoded_activation, decoder_name)\n",
        "    decoded = decoder(encoded)\n",
        "\n",
        "    autoencoder = Model(input_img, decoded, name='autoencoder')\n",
        "\n",
        "    # Training\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=autoencoder_learning_rate)\n",
        "    train_MAE_metric = keras.metrics.MeanAbsoluteError()\n",
        "    val_MAE_metric = keras.metrics.MeanAbsoluteError()\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train_norm, y_train)).shuffle(512).batch(autoencoder_batch_size)\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test_norm, y_test)).batch(autoencoder_batch_size)\n",
        "\n",
        "    train_losses = []\n",
        "    train_MAEs = []\n",
        "    val_MAEs = []\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(x, y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            decoder_preds = autoencoder(x, training=True)\n",
        "            if reconstruct_with_HFENN:\n",
        "\n",
        "                loss_value_hfenn = hfenn(x, decoder_preds)\n",
        "                loss_value_reconstruction = reconstruction_loss(x, decoder_preds)\n",
        "                loss = (lambdaHFENN * loss_value_hfenn) + (lambdaR * loss_value_reconstruction)\n",
        "            else:\n",
        "                loss = lambdaR * reconstruction_loss(x, decoder_preds)\n",
        "\n",
        "        grads = tape.gradient(loss, autoencoder.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, autoencoder.trainable_weights))\n",
        "        train_MAE_metric.update_state(x, decoder_preds)\n",
        "        return loss\n",
        "\n",
        "    @tf.function\n",
        "    def test_step(x, y):\n",
        "        val_preds = autoencoder(x, training=False)\n",
        "        val_MAE_metric.update_state(x, val_preds)\n",
        "\n",
        "    epoch = 0\n",
        "    epochs_wo_improving = 0\n",
        "    reconstruction_convergence = 0\n",
        "\n",
        "    while reconstruction_convergence == 0:\n",
        "        print(f\"\\nEpoch {epoch}\")\n",
        "        for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "            loss_value = train_step(x_batch_train, y_batch_train)\n",
        "            if step % 100 == 0:\n",
        "                print(f\"Step {step}: loss = {float(loss_value):.4f}\")\n",
        "\n",
        "        train_MAE = train_MAE_metric.result()\n",
        "        val_MAE = 0.0\n",
        "        train_MAE_metric.reset_state()\n",
        "\n",
        "        for x_val, y_val in test_dataset:\n",
        "            test_step(x_val, y_val)\n",
        "\n",
        "        val_MAE = val_MAE_metric.result()\n",
        "        val_MAE_metric.reset_state()\n",
        "\n",
        "        print(f\"Train MAE: {train_MAE:.4f} | Val MAE: {val_MAE:.4f}\")\n",
        "\n",
        "        train_losses.append(float(loss_value))\n",
        "        train_MAEs.append(float(train_MAE))\n",
        "        val_MAEs.append(float(val_MAE))\n",
        "\n",
        "        if epoch == 0:\n",
        "            best_val_MAE = val_MAE\n",
        "        elif val_MAE < best_val_MAE:\n",
        "            best_val_MAE = val_MAE\n",
        "            epochs_wo_improving = 0\n",
        "            encoder.save(store_encoder_path)\n",
        "            decoder.save(store_decoder_path)\n",
        "            autoencoder.save(store_AE_path)\n",
        "        else:\n",
        "            epochs_wo_improving += 1\n",
        "\n",
        "        if reconstruction_patience_epochs < epochs_wo_improving and epoch != 0:\n",
        "            reconstruction_convergence = 1\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "    print(f\"✅ Finished training for Speaker {speaker} in {epoch} epochs.\")"
      ],
      "metadata": {
        "id": "HzkqD94Vq0wr"
      },
      "id": "HzkqD94Vq0wr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Evaluation**"
      ],
      "metadata": {
        "id": "_hp7m_ATuDvY"
      },
      "id": "_hp7m_ATuDvY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Functions"
      ],
      "metadata": {
        "id": "h5oS_o7cudsY"
      },
      "id": "h5oS_o7cudsY"
    },
    {
      "cell_type": "code",
      "source": [
        "def mel_to_audio(mel_spec):\n",
        "    mel_spec = librosa.db_to_power(mel_spec)\n",
        "    audio = librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr,\n",
        "                                                 hop_length=hop_length,\n",
        "                                                 window=window,\n",
        "                                                 win_length=win_length,\n",
        "                                                 n_fft=n_fft,\n",
        "                                                 )\n",
        "    return audio"
      ],
      "metadata": {
        "id": "8tkFMxGcubX4"
      },
      "id": "8tkFMxGcubX4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models(speaker):\n",
        "    stored_autoencoder_path = f'/home/jovyan/Daniel/Exp_{dataset}/autoencoders_training/models/autoencoder_Speaker{speaker}.keras'\n",
        "    stored_encoder_path = f'/home/jovyan/Daniel/Exp_{dataset}/autoencoders_training/models/encoder_Speaker{speaker}.keras'\n",
        "    stored_decoder_path = f'/home/jovyan/Daniel/Exp_{dataset}/autoencoders_training/models/decoder_Speaker{speaker}.keras'\n",
        "\n",
        "    autoencoder = keras.models.load_model(stored_autoencoder_path)\n",
        "    encoder = keras.models.load_model(stored_encoder_path)\n",
        "    decoder = keras.models.load_model(stored_decoder_path)\n",
        "\n",
        "    return autoencoder, encoder, decoder"
      ],
      "metadata": {
        "id": "_EqjBDYEuFQA"
      },
      "id": "_EqjBDYEuFQA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def displayReconstructionVertical(speaker, plt_name, n, x_to_show, decoder_preds, n_frames_cnn, n_freq_cnn, displayEdges=False, display_sobel=False, LoG=False):\n",
        "    size = 'big'\n",
        "    fontsize = 48\n",
        "    if size == 'small':\n",
        "        size_x = (n_freq_cnn / 3) / 10\n",
        "        size_y = n_frames_cnn / 10\n",
        "    elif size == 'big':\n",
        "        size_x = (n_freq_cnn * 6) / 2\n",
        "        size_y = n_frames_cnn / 2\n",
        "\n",
        "    num_imgs_per_row = n\n",
        "    columns = n // int(num_imgs_per_row/2)\n",
        "    show = 'img'\n",
        "\n",
        "    if not displayEdges:\n",
        "      scale = 'Hz'\n",
        "    else:\n",
        "      scale = 'Mel'\n",
        "    colorbar = False\n",
        "\n",
        "    num_show = 0\n",
        "\n",
        "    for i in range(columns):\n",
        "        fig = plt.figure(figsize=(size_x, size_y))\n",
        "\n",
        "        for j in range(num_imgs_per_row):\n",
        "            ax = plt.subplot(2, num_imgs_per_row, j + 1)  # Change to 2 rows per figure\n",
        "\n",
        "            if show == 'img':\n",
        "                image = x_to_show[num_show]\n",
        "                title = r\"$X_{{{}}}$\".format(num_show)\n",
        "            if show == 'recons':\n",
        "                if displayEdges:\n",
        "                  if display_sobel:\n",
        "                    x = K.expand_dims(x_to_show,-1)\n",
        "                    x_tensor = tf.convert_to_tensor(x)\n",
        "                    sobel = tf.image.sobel_edges(x_tensor)\n",
        "                    sobel_x = sobel[..., 0]  # Horizontal edges\n",
        "                    sobel_y = sobel[..., 1]  # Vertical edges\n",
        "                    image = tf.sqrt(tf.square(sobel_x[num_show]) + tf.square(sobel_y[num_show]))\n",
        "                    title = r\"$G(X_{{{}}})$\".format(num_show)\n",
        "                  if LoG:\n",
        "                    x = x_to_show[num_show].reshape(n_frames_cnn, n_freq_cnn)\n",
        "                    blurred = cv2.GaussianBlur(x, (5, 5), 0)  # Apply Gaussian Blur\n",
        "                    image = cv2.Laplacian(blurred, cv2.CV_64F)  # Apply Laplacian filter\n",
        "                    title = r\"\"\"$LoG(X_{})$\"\"\".format(num_show)\n",
        "                else:\n",
        "                  image = decoder_preds[num_show]\n",
        "                  title = r\"$\\tilde{{X}}_{{{}}}$\".format(num_show)\n",
        "                num_show += 1\n",
        "\n",
        "            if scale == 'Mel':\n",
        "                if show=='recons':\n",
        "                  im = plt.imshow(image, origin='lower')\n",
        "                else:\n",
        "                  im = plt.imshow(image.reshape(n_frames_cnn, n_freq_cnn), origin='lower')\n",
        "\n",
        "                plt.title(title, fontsize=fontsize)\n",
        "\n",
        "                if colorbar or j==(num_imgs_per_row-1):\n",
        "                    divider = make_axes_locatable(ax)\n",
        "                    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
        "                    plt.colorbar(im, cax=cax, format=\"%+2.f dB\")\n",
        "\n",
        "            if scale == 'Hz':\n",
        "                im = librosa.display.specshow(image, cmap='viridis', x_axis='time', y_axis='mel', sr=48000)\n",
        "\n",
        "                plt.title(title, fontsize=fontsize)\n",
        "\n",
        "                if colorbar or j==(num_imgs_per_row-1):\n",
        "                    divider = make_axes_locatable(ax)\n",
        "                    cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
        "                    plt.colorbar(im, cax=cax, format=\"%+2.f dB\")\n",
        "\n",
        "            plt.xticks(fontsize=fontsize)\n",
        "            plt.yticks(fontsize=fontsize)\n",
        "            ax.get_xaxis().set_ticks([])\n",
        "            ax.set_xlabel('Time', fontsize=fontsize)\n",
        "            if displayEdges:\n",
        "              ax.set_ylabel('Mel', fontsize=fontsize)\n",
        "            else:\n",
        "              ax.set_ylabel('Hz', fontsize=fontsize)\n",
        "            if j > 0:\n",
        "              ax.get_yaxis().set_visible(False)\n",
        "\n",
        "            if show == 'img':\n",
        "              show = 'recons'\n",
        "            else:\n",
        "              show ='img'\n",
        "\n",
        "        plt.savefig(f'/home/jovyan/Daniel/Exp_{dataset}/autoencoders_training/evaluation/Spe{speaker}_{plt_name}_{i}.png')\n",
        "        plt.show()\n",
        "        plt.close()"
      ],
      "metadata": {
        "id": "RGeS6CuouYyY"
      },
      "id": "RGeS6CuouYyY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Loop"
      ],
      "metadata": {
        "id": "eOTeOd8IueoA"
      },
      "id": "eOTeOd8IueoA"
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "#for speaker_id in range(1, n_speakers + 1):\n",
        "for speaker in speakers:\n",
        "    speaker = f\"{speaker_id:02d}\"  # format as '01', '02', ..., '50'\n",
        "    print(f\"\\n\\n🔁 Evaluating Autoencoder for Speaker {speaker}...\")\n",
        "\n",
        "    # Define paths\n",
        "    data_path = f'/home/jovyan/Daniel/Exp_{dataset}/datasets/preprocessed/{dataset}100ms32LabeledSpe{speaker}.npz'\n",
        "\n",
        "    # Load data\n",
        "    data = np.load(data_path)\n",
        "    x_train_100ms = data['x_train']\n",
        "    y_train_100ms = data['y_emo']\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        x_train_100ms, y_train_100ms, test_size=test_prctg, random_state=42)\n",
        "\n",
        "    # Normalization\n",
        "    min_norm = np.min(x_train)\n",
        "    max_norm = np.max(x_train)\n",
        "\n",
        "    x_train_norm = (x_train - min_norm) / (max_norm - min_norm)\n",
        "    x_test_norm = (x_test - min_norm) / (max_norm - min_norm)\n",
        "\n",
        "    # Load models\n",
        "    autoencoder, encoder, decoder = load_models(speaker)\n",
        "\n",
        "    autoencoder_mae_metric = keras.metrics.MeanAbsoluteError()\n",
        "    autoencoder.compile(optimizer='adam', loss='mean_squared_error', metrics=[autoencoder_mae_metric])\n",
        "    autoencoder_mae = autoencoder.evaluate(x_test_norm, x_test_norm, verbose=0)\n",
        "    mae_value = autoencoder_mae[1]\n",
        "    print(f\"Autoencoder MAE: {mae_value}\")\n",
        "\n",
        "    # Store the result\n",
        "    results.append({'Speaker': speaker, 'MAE': mae_value})\n",
        "\n",
        "    # Qualitative assessment\n",
        "    n=12\n",
        "    x_to_show = x_test_norm[:n]\n",
        "    decoder_preds = autoencoder.predict(x_to_show)\n",
        "    reconstructions_rescaled = decoder_preds * (max_norm - min_norm) + min_norm\n",
        "\n",
        "    displayReconstructionVertical(speaker=speaker,\n",
        "                                  plt_name='reconstruction',\n",
        "                                  n=n,\n",
        "                                  x_to_show=x_to_show,\n",
        "                                  decoder_preds=reconstructions_rescaled,\n",
        "                                  n_frames_cnn=x_train.shape[1],\n",
        "                                  n_freq_cnn=x_train.shape[2])\n",
        "\n",
        "    # Gradient loss display\n",
        "    n=12\n",
        "    x_to_show = x_train[:n]\n",
        "    decoder_preds = autoencoder.predict(x_to_show)\n",
        "\n",
        "    displayReconstructionVertical(speaker=speaker,\n",
        "                                  plt_name='sobel_edges',\n",
        "                                  n=n,\n",
        "                                  x_to_show=x_to_show,\n",
        "                                  decoder_preds=reconstructions_rescaled,\n",
        "                                  n_frames_cnn=x_train.shape[1],\n",
        "                                  n_freq_cnn=x_train.shape[2],\n",
        "                                  displayEdges = True,\n",
        "                                  display_sobel=True)\n",
        "\n",
        "    # Audio reconstruction\n",
        "    x_test_reconstructed = autoencoder.predict(x_test_norm)\n",
        "\n",
        "    for i, mel_spec in enumerate(x_test[:5]):\n",
        "        audio = mel_to_audio(mel_spec)\n",
        "        filename = f'audio_{i}.wav'\n",
        "        wavfile.write(filename, sr, audio)\n",
        "        display(Audio(filename))\n",
        "\n",
        "    reconstructions_rescaled = x_test_reconstructed * (max_norm - min_norm) + min_norm\n",
        "    print('Audio reconstruction through Inverse Fourier Transform:')\n",
        "\n",
        "    for i, mel_spec in enumerate(reconstructions_rescaled[:5]):\n",
        "        audio = mel_to_audio(mel_spec)\n",
        "        filename = f'recons_audio_{i}.wav'\n",
        "        wavfile.write(filename, sr, audio)\n",
        "        display(Audio(filename))\n",
        "\n",
        "    print(f\"✅ Finished evaluating Speaker {speaker}.\")"
      ],
      "metadata": {
        "id": "3O6h9aH6ufuH"
      },
      "id": "3O6h9aH6ufuH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "results_csv_path = f'/home/jovyan/Daniel/Exp_{dataset}/autoencoders_training/evaluation/mae_results.csv'\n",
        "results_df.to_csv(results_csv_path, index=False)\n",
        "print(f\"Saved MAE results table to {results_csv_path}\")\n",
        "\n",
        "# Plot the table as an image\n",
        "fig, ax = plt.subplots(figsize=(8, len(results_df) * 0.3 + 1))\n",
        "ax.axis('off')\n",
        "table = ax.table(cellText=results_df.values,\n",
        "                 colLabels=results_df.columns,\n",
        "                 cellLoc='center',\n",
        "                 loc='center')\n",
        "table.scale(1, 1.5)\n",
        "\n",
        "# Save the image\n",
        "image_path = f'/home/jovyan/Daniel/Exp_{dataset}/autoencoders_training/evaluation/mae_results_table.png'\n",
        "plt.savefig(image_path, bbox_inches='tight')\n",
        "plt.close()\n",
        "\n",
        "print(f\"Saved MAE table image to {image_path}\")"
      ],
      "metadata": {
        "id": "yV5-8O50wCZX"
      },
      "id": "yV5-8O50wCZX",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "62787479",
        "e7ee0eaa",
        "rtA7EQfYOt6-",
        "40be7729",
        "_hp7m_ATuDvY",
        "h5oS_o7cudsY",
        "eOTeOd8IueoA"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
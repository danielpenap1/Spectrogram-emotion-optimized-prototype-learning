# ğŸ“„ Explainable Audio Emotion Recognition Using High-Resolution Autoencoder and Prototype-Based Deep Learning
### Daniel PeÃ±a Porras, Esteban GarcÃ­a-Cuesta

---

This repository accompanies the journal paper:

> **Explainable Audio Emotion Recognition Using High-Resolution Autoencoder and Prototype-Based Deep Learning**  
> *Authors:* Daniel PeÃ±a, Esteban GarcÃ­a-Cuesta.  
> *Journal:* [Name of the Journal], [Year]
> *DOI:* [DOI]  

The repository complements and extends the material included in the article. It also provides the necessary code to run the experiments as well as the explainability results: the learned **prototypes** as spectrograms and as audio signals.

---

## âš™ï¸ Repository Structure and Usage

In the ğŸ“‚**prototypes** folder, for each of the three datasets (EmoMatchSpanishDB, RAVDESS and EmoDB), we present the top-10 prototypes of each emotion of four different speakers. In the ğŸ“‚**figures** folder we include the rest of the figures describing the model and the results.

The repository includes both **Jupyter notebooks** (`.ipynb`) and equivalent **Python scripts** (`.py`) for the entire experimental pipeline. You can execute either version depending on your workflow preference.

The project is organized into **four main stages**. Each notebook can be executed independently by simply setting the appropriate dataset path.

1. **`0_preprocess.ipynb`** â€“ Data loading and spectrogram generation.
2. **`1_autoencoders_training_and_evaluation.ipynb`** â€“ Train and evaluate the high-resolution autoencoders for the different speakers.
3. **`2_prototype_classifiers_training_and_evaluation.ipynb`** â€“ Train the prototype-based deep learning classifiers.
4. **`3_prototype_analysis.ipynb`** â€“ Analyze learned prototypes and generate visualizations.

> ğŸ’¡ *Tip:* These notebooks are self-contained and can be adapted for different datasets by modifying the dataset path at the beginning of each notebook.

---

## ğŸŒ Complementary Webpage

An interactive GitHub Page is provided:

ğŸ”— **[https://danielpenap1.github.io/Spectrogram-emotion-optimized-prototype-learning/](https://danielpenap1.github.io/Spectrogram-emotion-optimized-prototype-learning/)**

The page provides the reproducible prototypes in foldable sections for each dataset and speaker. It allows:
- Visualization of **learned spectrogram prototypes** per emotion
- Reproduction of **audio prototypes**  

---